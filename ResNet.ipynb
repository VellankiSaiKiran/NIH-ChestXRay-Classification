{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pE8aoL4E5f-l",
        "outputId": "ff1abede-a21f-4c66-a086-f66c357953b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import zipfile\n",
        "import os\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "folder_path = '/content/drive/My Drive/Capstone/Data/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "FlLVxjF857qA"
      },
      "outputs": [],
      "source": [
        "train_labels_path = '/content/drive/My Drive/Capstone/Data/train.xlsx'\n",
        "test_labels_path = '/content/drive/My Drive/Capstone/Data/test.xlsx'\n",
        "val_labels_path = '/content/drive/My Drive/Capstone/Data/validation.xlsx'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "t5AkdEZS6pCA"
      },
      "outputs": [],
      "source": [
        "# Load the label data from the Excel files\n",
        "train_df = pd.read_excel(train_labels_path)\n",
        "test_df = pd.read_excel(test_labels_path)\n",
        "val_df = pd.read_excel(val_labels_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "SZpt_ZhhLEhU",
        "outputId": "dfc1f8a7-cd1a-4cbc-e6d3-86c7979ed252"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          class                  image\n",
              "0          Mass  00013285_013_3581.png\n",
              "1          Mass  00005785_000_2597.png\n",
              "2          Mass       00019369_002.png\n",
              "3          Mass       00012342_003.png\n",
              "4          Mass       00018663_002.png\n",
              "...         ...                    ...\n",
              "95995  Fibrosis  00002437_035_7392.png\n",
              "95996  Fibrosis  00006043_000_3247.png\n",
              "95997  Fibrosis  00003098_012_5153.png\n",
              "95998  Fibrosis  00000149_007_5731.png\n",
              "95999  Fibrosis  00016684_000_1504.png\n",
              "\n",
              "[96000 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0eecfc4c-13b0-477b-94c3-31b1d579c74d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>class</th>\n",
              "      <th>image</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Mass</td>\n",
              "      <td>00013285_013_3581.png</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Mass</td>\n",
              "      <td>00005785_000_2597.png</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Mass</td>\n",
              "      <td>00019369_002.png</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Mass</td>\n",
              "      <td>00012342_003.png</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Mass</td>\n",
              "      <td>00018663_002.png</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95995</th>\n",
              "      <td>Fibrosis</td>\n",
              "      <td>00002437_035_7392.png</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95996</th>\n",
              "      <td>Fibrosis</td>\n",
              "      <td>00006043_000_3247.png</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95997</th>\n",
              "      <td>Fibrosis</td>\n",
              "      <td>00003098_012_5153.png</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95998</th>\n",
              "      <td>Fibrosis</td>\n",
              "      <td>00000149_007_5731.png</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95999</th>\n",
              "      <td>Fibrosis</td>\n",
              "      <td>00016684_000_1504.png</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>96000 rows Ã— 2 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0eecfc4c-13b0-477b-94c3-31b1d579c74d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-0eecfc4c-13b0-477b-94c3-31b1d579c74d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-0eecfc4c-13b0-477b-94c3-31b1d579c74d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-dfe2c038-0923-46cc-85ce-88e58fc0e708\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-dfe2c038-0923-46cc-85ce-88e58fc0e708')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-dfe2c038-0923-46cc-85ce-88e58fc0e708 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_e863e0cb-a435-4ded-99ec-0b682bf56e12\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('train_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_e863e0cb-a435-4ded-99ec-0b682bf56e12 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('train_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "train_df",
              "summary": "{\n  \"name\": \"train_df\",\n  \"rows\": 96000,\n  \"fields\": [\n    {\n      \"column\": \"class\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 15,\n        \"samples\": [\n          \"Edema\",\n          \"Emphysema\",\n          \"Mass\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"image\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 85334,\n        \"samples\": [\n          \"00008468_040_779.png\",\n          \"00003129_009.png\",\n          \"00014022_102_1966.png\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "train_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NglaZ-PEnazO",
        "outputId": "2b569c6b-b6da-4778-b57f-ac10eac1917f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 1min 22s, sys: 51.8 s, total: 2min 14s\n",
            "Wall time: 1min 48s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "import zipfile\n",
        "import concurrent.futures\n",
        "zf = zipfile.ZipFile('/content/drive/My Drive/Capstone/Data/TrainTestVal-Images.zip')\n",
        "def unzip(file):\n",
        "    zf.extract(file)\n",
        "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "    executor.map(unzip, zf.infolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4amZVsCYmSI3",
        "outputId": "97d24854-9096-4c70-cddb-e368e287cabc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The folder contains 85334 image(s).\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Replace with the actual path to your images folder\n",
        "images_folder_path = '/content/TrainTestVal-Images/Train'\n",
        "\n",
        "# List of image file extensions we're looking for\n",
        "image_extensions = ['.jpg', '.jpeg', '.png', '.gif', '.bmp', '.tiff', '.webp']\n",
        "\n",
        "# Initialize a counter\n",
        "image_count = 0\n",
        "\n",
        "# Walk through all files and folders within the images folder\n",
        "for root, dirs, files in os.walk(images_folder_path):\n",
        "    for file in files:\n",
        "        # Check if the file has one of the image file extensions\n",
        "        if any(file.lower().endswith(ext) for ext in image_extensions):\n",
        "            image_count += 1\n",
        "\n",
        "print(f'The folder contains {image_count} image(s).')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CUOa00D_q3qa",
        "outputId": "ea2bc9c3-075a-4fa3-a38e-317672a55fed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All training files are present.\n",
            "All test files are present.\n",
            "All validation files are present.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Define the paths to your image directories and annotation files\n",
        "train_annotations_path = '/content/drive/My Drive/Capstone/Data/train.xlsx'\n",
        "test_annotations_path = '/content/drive/My Drive/Capstone/Data/test.xlsx'\n",
        "validation_annotations_path = '/content/drive/My Drive/Capstone/Data/validation.xlsx'\n",
        "\n",
        "train_img_dir = '/content/TrainTestVal-Images/Train'\n",
        "test_img_dir = '/content/TrainTestVal-Images/Test'\n",
        "validation_img_dir = '/content/TrainTestVal-Images/Validation'\n",
        "\n",
        "# A function to check if files exist\n",
        "def check_images_exist(annotations_path, img_dir):\n",
        "    # Read the annotations file\n",
        "    df = pd.read_excel(annotations_path)\n",
        "    # Assuming the Excel file has a column named 'filename' with the image file names\n",
        "    missing_files = []\n",
        "    for filename in df['image']:\n",
        "        file_path = os.path.join(img_dir, filename)\n",
        "        if not os.path.isfile(file_path):\n",
        "            missing_files.append(filename)\n",
        "    return missing_files\n",
        "\n",
        "# Check each dataset\n",
        "missing_train_files = check_images_exist(train_annotations_path, train_img_dir)\n",
        "missing_test_files = check_images_exist(test_annotations_path, test_img_dir)\n",
        "missing_validation_files = check_images_exist(validation_annotations_path, validation_img_dir)\n",
        "\n",
        "# Report the results\n",
        "if missing_train_files:\n",
        "    print(f'Missing training files: {missing_train_files}')\n",
        "else:\n",
        "    print('All training files are present.')\n",
        "\n",
        "if missing_test_files:\n",
        "    print(f'Missing test files: {missing_test_files}')\n",
        "else:\n",
        "    print('All test files are present.')\n",
        "\n",
        "if missing_validation_files:\n",
        "    print(f'Missing validation files: {missing_validation_files}')\n",
        "else:\n",
        "    print('All validation files are present.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "JkAeEGyeohlQ"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "class CustomImageDataset(Dataset):\n",
        "    def __init__(self, annotations_file, img_dir, transform=None):\n",
        "        self.img_labels = pd.read_excel(annotations_file)\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "      img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 1])\n",
        "      image = Image.open(img_path).convert('RGB')  # Convert image to RGB\n",
        "      label = self.img_labels.iloc[idx, 0]\n",
        "      # Convert the label from string to integer using the mapping dictionary\n",
        "      label = class_to_idx[label]\n",
        "      if self.transform:\n",
        "          image = self.transform(image)\n",
        "      return image, label\n",
        "\n",
        "# Define transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(224),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = CustomImageDataset(annotations_file='/content/drive/My Drive/Capstone/Data/train.xlsx',\n",
        "                                   img_dir='/content/TrainTestVal-Images/Train',\n",
        "                                   transform=transform)\n",
        "\n",
        "test_dataset = CustomImageDataset(annotations_file='/content/drive/My Drive/Capstone/Data/test.xlsx',\n",
        "                                  img_dir='/content/TrainTestVal-Images/Test',\n",
        "                                  transform=transform)\n",
        "\n",
        "val_dataset = CustomImageDataset(annotations_file='/content/drive/My Drive/Capstone/Data/validation.xlsx',\n",
        "                                 img_dir='/content/TrainTestVal-Images/Validation',\n",
        "                                 transform=transform)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class_to_idx = {class_name: index for index, class_name in enumerate(pd.unique(train_dataset.img_labels['class']))}\n",
        "class_to_idx\n",
        "\n",
        "\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)\n",
        "val_loader = DataLoader(dataset=val_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rndiLmio4qIu"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0jeHnCElAv8"
      },
      "source": [
        "## FIRST MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L2c0v5syd8tM",
        "outputId": "774aa850-2411-4a15-d9bd-7215ec38a595"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train Loss: 2.4384 Acc: 0.1921\n",
            "val Loss: 2.5763 Acc: 0.1953\n",
            "train Loss: 2.2719 Acc: 0.2444\n",
            "val Loss: 2.5833 Acc: 0.2219\n",
            "train Loss: 2.2115 Acc: 0.2617\n",
            "val Loss: 2.2411 Acc: 0.2586\n",
            "train Loss: 2.1725 Acc: 0.2744\n",
            "val Loss: 2.2408 Acc: 0.2542\n",
            "train Loss: 2.1340 Acc: 0.2850\n",
            "val Loss: 2.3105 Acc: 0.2568\n",
            "train Loss: 2.1035 Acc: 0.2937\n",
            "val Loss: 3.3217 Acc: 0.2468\n",
            "train Loss: 2.0821 Acc: 0.3005\n",
            "val Loss: 3.2603 Acc: 0.2120\n",
            "train Loss: 1.9682 Acc: 0.3334\n",
            "val Loss: 1.9926 Acc: 0.3277\n",
            "train Loss: 1.9226 Acc: 0.3498\n",
            "val Loss: 1.9892 Acc: 0.3306\n",
            "train Loss: 1.8974 Acc: 0.3556\n",
            "val Loss: 1.9583 Acc: 0.3399\n",
            "train Loss: 1.8793 Acc: 0.3601\n",
            "val Loss: 1.9512 Acc: 0.3419\n",
            "train Loss: 1.8632 Acc: 0.3638\n",
            "val Loss: 1.9890 Acc: 0.3378\n",
            "train Loss: 1.8399 Acc: 0.3730\n",
            "val Loss: 1.9451 Acc: 0.3440\n",
            "train Loss: 1.8197 Acc: 0.3785\n",
            "val Loss: 1.9164 Acc: 0.3505\n",
            "train Loss: 1.7795 Acc: 0.3906\n",
            "val Loss: 1.8956 Acc: 0.3613\n",
            "train Loss: 1.7705 Acc: 0.3938\n",
            "val Loss: 1.8947 Acc: 0.3586\n",
            "train Loss: 1.7665 Acc: 0.3971\n",
            "val Loss: 1.8889 Acc: 0.3603\n",
            "train Loss: 1.7594 Acc: 0.3972\n",
            "val Loss: 1.8879 Acc: 0.3615\n",
            "train Loss: 1.7520 Acc: 0.4007\n",
            "val Loss: 1.8825 Acc: 0.3596\n",
            "train Loss: 1.7492 Acc: 0.4036\n",
            "val Loss: 1.8785 Acc: 0.3599\n",
            "train Loss: 1.7470 Acc: 0.4014\n",
            "val Loss: 1.8767 Acc: 0.3644\n",
            "train Loss: 1.7422 Acc: 0.4043\n",
            "val Loss: 1.8801 Acc: 0.3652\n",
            "train Loss: 1.7398 Acc: 0.4039\n",
            "val Loss: 1.8849 Acc: 0.3583\n",
            "train Loss: 1.7382 Acc: 0.4054\n",
            "val Loss: 1.8791 Acc: 0.3662\n",
            "train Loss: 1.7371 Acc: 0.4035\n",
            "val Loss: 1.8846 Acc: 0.3641\n",
            "Best val Acc: 0.3662\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (4): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (5): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=2048, out_features=15, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import models\n",
        "from torchsummary import summary\n",
        "# Import the copy module\n",
        "import copy\n",
        "\n",
        "# Check if GPU is available and set the device accordingly\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load the pretrained ResNet model\n",
        "model = models.resnet50(pretrained=True)\n",
        "\n",
        "\n",
        "num_classes = 15\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Linear(num_ftrs, num_classes)\n",
        "\n",
        "# Move the model to the GPU if available\n",
        "model = model.to(device)\n",
        "\n",
        "# Summary of the model (optional, it requires torchsummary to be installed)\n",
        "# summary(model, input_size=(3, 224, 224))\n",
        "\n",
        "# Assume the rest of the setup code above remains the same...\n",
        "\n",
        "# Define the loss function, optimizer, and learning rate scheduler\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
        "\n",
        "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
        "    best_acc = 0.0\n",
        "    for epoch in range(num_epochs):\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "                dataloader = train_loader\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "                dataloader = val_loader\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for inputs, labels in dataloader:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # Zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # Forward\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    # Backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # Statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            if phase == 'train':\n",
        "                scheduler.step()\n",
        "\n",
        "            epoch_loss = running_loss / len(dataloader.dataset)\n",
        "            epoch_acc = running_corrects.double() / len(dataloader.dataset)\n",
        "\n",
        "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "    print(f'Best val Acc: {best_acc:.4f}')\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model\n",
        "\n",
        "# Train the model\n",
        "model_trained = train_model(model, criterion, optimizer, lr_scheduler, num_epochs=30)\n",
        "\n",
        "# Save the model (optional)\n",
        "torch.save(model_trained.state_dict(), '/content/drive/My Drive/Capstone/model_resnet50.pth')\n",
        "\n",
        "\n",
        "# Save the model (optional)\n",
        "# torch.save(model_trained.state_dict(), 'model_resnet50.pth')\n",
        "\n",
        "# To evaluate on the test set\n",
        "model.eval()\n",
        "# Then loop over your test_loader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8eDTTXjXq9hb",
        "outputId": "60303b63-b015-4c11-90c6-e863f9154cae"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Loss: 1.8935 Acc: 0.3554\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import models\n",
        "# Import the copy module\n",
        "import copy\n",
        "\n",
        "# Define the device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Define the model architecture (must be the same as when the model was saved)\n",
        "num_classes = 15  # Make sure this is the same as when you trained the model\n",
        "model = models.resnet50(pretrained=False)  # pretrained=False since we're loading our trained weights\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Linear(num_ftrs, num_classes)\n",
        "\n",
        "# Load the saved weights\n",
        "model.load_state_dict(torch.load('/content/drive/My Drive/Capstone/model_resnet50.pth', map_location=device))\n",
        "\n",
        "# Move the model to the GPU if available\n",
        "model = model.to(device)\n",
        "\n",
        "# Define your criterion here (should be the same as the one used during training)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Assuming your DataLoader is named 'test_loader'\n",
        "# Replace 'your_test_loader' with the actual variable name of your test DataLoader\n",
        "\n",
        "# Function to evaluate the model\n",
        "def evaluate_model(model, test_loader, criterion):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "\n",
        "    # No need to track gradients for validation\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # Forward pass to get outputs\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Calculate the loss\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Get predictions\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            # Update running loss and accuracy\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "    # Calculate average loss and accuracy over all test data\n",
        "    total_loss = running_loss / len(test_loader.dataset)\n",
        "    total_acc = running_corrects.double() / len(test_loader.dataset)\n",
        "\n",
        "    print(f'Test Loss: {total_loss:.4f} Acc: {total_acc:.4f}')\n",
        "\n",
        "# Evaluate the model using the test dataset\n",
        "evaluate_model(model, test_loader, criterion)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4NS_t-3ieSoc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgqHUX3sk6Ey"
      },
      "source": [
        "##SECOND MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BHq8_IcCttgx",
        "outputId": "18e5162b-4800-48b7-ff50-4a4f407121d5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Mass': 0,\n",
              " 'Cardiomegaly': 1,\n",
              " 'Atelectasis': 2,\n",
              " 'Effusion': 3,\n",
              " 'Pneumothorax': 4,\n",
              " 'No Finding': 5,\n",
              " 'Subcutaneous Emphysema': 6,\n",
              " 'Nodule': 7,\n",
              " 'Pleural Thickening': 8,\n",
              " 'Edema': 9,\n",
              " 'Pneumonia': 10,\n",
              " 'Emphysema': 11,\n",
              " 'Infiltration': 12,\n",
              " 'Consolidation': 13,\n",
              " 'Fibrosis': 14}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "class_to_idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3VQ2qk0Vtteh"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "hV_6uXG4llSN"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "class CustomImageDataset(Dataset):\n",
        "    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
        "        self.img_labels = pd.read_excel(annotations_file)\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 1])\n",
        "        image = Image.open(img_path).convert('RGB')  # Convert image to RGB\n",
        "        label = self.img_labels.iloc[idx, 0]\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        if self.target_transform:\n",
        "            label = self.target_transform(label)\n",
        "        return image, label\n",
        "\n",
        "# Define transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(224),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Assuming you have a function to transform your labels\n",
        "def target_transform(label):\n",
        "    # Your code to transform the label if necessary, e.g., a class to index mapping\n",
        "    return class_to_idx[label]\n",
        "\n",
        "# You would need to create a mapping dictionary similar to what you have previously\n",
        "\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = CustomImageDataset(\n",
        "    annotations_file='/content/drive/My Drive/Capstone/Data/train.xlsx',\n",
        "    img_dir='/content/TrainTestVal-Images/Train',\n",
        "    transform=transform,\n",
        "    target_transform=target_transform\n",
        ")\n",
        "\n",
        "test_dataset = CustomImageDataset(\n",
        "    annotations_file='/content/drive/My Drive/Capstone/Data/test.xlsx',\n",
        "    img_dir='/content/TrainTestVal-Images/Test',\n",
        "    transform=transform,\n",
        "    target_transform=target_transform\n",
        ")\n",
        "\n",
        "\n",
        "# Create the validation dataset\n",
        "val_dataset = CustomImageDataset(\n",
        "    annotations_file='/content/drive/My Drive/Capstone/Data/validation.xlsx',\n",
        "    img_dir='/content/TrainTestVal-Images/Validation',\n",
        "    transform=transform,\n",
        "    target_transform=target_transform\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "# Create DataLoader\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aCcToOzillQE",
        "outputId": "319f6495-72f3-41b1-921b-a2a9becaeda6"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97.8M/97.8M [00:00<00:00, 182MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "----------\n",
            "train Loss: 2.4345 Acc: 0.1894\n",
            "val Loss: 2.3770 Acc: 0.2170\n",
            "Epoch 2/25\n",
            "----------\n",
            "train Loss: 2.2649 Acc: 0.2416\n",
            "val Loss: 2.2739 Acc: 0.2404\n",
            "Epoch 3/25\n",
            "----------\n",
            "train Loss: 2.1894 Acc: 0.2647\n",
            "val Loss: 2.1916 Acc: 0.2669\n",
            "Epoch 4/25\n",
            "----------\n",
            "train Loss: 2.1421 Acc: 0.2790\n",
            "val Loss: 2.1702 Acc: 0.2780\n",
            "Epoch 5/25\n",
            "----------\n",
            "train Loss: 2.1008 Acc: 0.2929\n",
            "val Loss: 2.3988 Acc: 0.2602\n",
            "Epoch 6/25\n",
            "----------\n",
            "train Loss: 2.0592 Acc: 0.3061\n",
            "val Loss: 2.1251 Acc: 0.2932\n",
            "Epoch 7/25\n",
            "----------\n",
            "train Loss: 2.0265 Acc: 0.3143\n",
            "val Loss: 2.1169 Acc: 0.2915\n",
            "Epoch 8/25\n",
            "----------\n",
            "train Loss: 1.9377 Acc: 0.3446\n",
            "val Loss: 1.9939 Acc: 0.3274\n",
            "Epoch 9/25\n",
            "----------\n",
            "train Loss: 1.9132 Acc: 0.3517\n",
            "val Loss: 1.9827 Acc: 0.3250\n",
            "Epoch 10/25\n",
            "----------\n",
            "train Loss: 1.9029 Acc: 0.3543\n",
            "val Loss: 1.9781 Acc: 0.3328\n",
            "Epoch 11/25\n",
            "----------\n",
            "train Loss: 1.8930 Acc: 0.3582\n",
            "val Loss: 1.9696 Acc: 0.3334\n",
            "Epoch 12/25\n",
            "----------\n",
            "train Loss: 1.8791 Acc: 0.3617\n",
            "val Loss: 1.9635 Acc: 0.3337\n",
            "Epoch 13/25\n",
            "----------\n",
            "train Loss: 1.8704 Acc: 0.3637\n",
            "val Loss: 1.9606 Acc: 0.3366\n",
            "Epoch 14/25\n",
            "----------\n",
            "train Loss: 1.8597 Acc: 0.3662\n",
            "val Loss: 1.9529 Acc: 0.3339\n",
            "Epoch 15/25\n",
            "----------\n",
            "train Loss: 1.8495 Acc: 0.3713\n",
            "val Loss: 1.9435 Acc: 0.3438\n",
            "Epoch 16/25\n",
            "----------\n",
            "train Loss: 1.8472 Acc: 0.3734\n",
            "val Loss: 1.9402 Acc: 0.3407\n",
            "Epoch 17/25\n",
            "----------\n",
            "train Loss: 1.8447 Acc: 0.3728\n",
            "val Loss: 1.9458 Acc: 0.3418\n",
            "Epoch 18/25\n",
            "----------\n",
            "train Loss: 1.8420 Acc: 0.3735\n",
            "val Loss: 1.9489 Acc: 0.3400\n",
            "Epoch 19/25\n",
            "----------\n",
            "train Loss: 1.8408 Acc: 0.3738\n",
            "val Loss: 1.9458 Acc: 0.3422\n",
            "Epoch 20/25\n",
            "----------\n",
            "train Loss: 1.8405 Acc: 0.3743\n",
            "val Loss: 1.9476 Acc: 0.3396\n",
            "Epoch 21/25\n",
            "----------\n",
            "train Loss: 1.8385 Acc: 0.3759\n",
            "val Loss: 1.9372 Acc: 0.3429\n",
            "Epoch 22/25\n",
            "----------\n",
            "train Loss: 1.8344 Acc: 0.3742\n",
            "val Loss: 1.9471 Acc: 0.3424\n",
            "Epoch 23/25\n",
            "----------\n",
            "train Loss: 1.8340 Acc: 0.3768\n",
            "val Loss: 1.9359 Acc: 0.3463\n",
            "Epoch 24/25\n",
            "----------\n",
            "train Loss: 1.8317 Acc: 0.3774\n",
            "val Loss: 1.9360 Acc: 0.3454\n",
            "Epoch 25/25\n",
            "----------\n",
            "train Loss: 1.8383 Acc: 0.3746\n",
            "val Loss: 1.9444 Acc: 0.3419\n",
            "Best val Acc: 0.3463\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import models\n",
        "import copy\n",
        "\n",
        "# Custom dataset\n",
        "class CustomImageDataset(Dataset):\n",
        "    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
        "        self.img_labels = pd.read_excel(annotations_file)\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 1])\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        label = self.img_labels.iloc[idx, 0]\n",
        "        if self.target_transform:\n",
        "            label = self.target_transform(label)\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "\n",
        "# Data transformations\n",
        "data_transforms = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(224),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Assuming you have a function or dictionary to transform your labels\n",
        "\n",
        "def target_transform(label):\n",
        "    return class_to_idx[label]\n",
        "\n",
        "# Initialize datasets\n",
        "train_dataset = CustomImageDataset(\n",
        "    annotations_file='/content/drive/My Drive/Capstone/Data/train.xlsx',\n",
        "    img_dir='/content/TrainTestVal-Images/Train',\n",
        "    transform=data_transforms,\n",
        "    target_transform=target_transform\n",
        ")\n",
        "\n",
        "val_dataset = CustomImageDataset(\n",
        "    annotations_file='/content/drive/My Drive/Capstone/Data/validation.xlsx',\n",
        "    img_dir='/content/TrainTestVal-Images/Validation',\n",
        "    transform=data_transforms,\n",
        "    target_transform=target_transform\n",
        ")\n",
        "\n",
        "# Initialize dataloaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "# Initialize the model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = models.resnet50(pretrained=True)\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Linear(num_ftrs, len(class_to_idx))  # Assuming you have 15 classes\n",
        "model = model.to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
        "\n",
        "# Training function\n",
        "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()\n",
        "            else:\n",
        "                model.eval()\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data\n",
        "            for inputs, labels in (train_loader if phase == 'train' else val_loader):\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # Zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # Forward\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    # Backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # Statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            if phase == 'train':\n",
        "                scheduler.step()\n",
        "\n",
        "            epoch_loss = running_loss / len(train_loader.dataset if phase == 'train' else val_loader.dataset)\n",
        "            epoch_acc = running_corrects.double() / len(train_loader.dataset if phase == 'train' else val_loader.dataset)\n",
        "\n",
        "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
        "\n",
        "            # Deep copy the model\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "    print(f'Best val Acc: {best_acc:.4f}')\n",
        "    # Load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model\n",
        "\n",
        "# Train the model\n",
        "model_ft = train_model(model, criterion, optimizer, scheduler, num_epochs=25)\n",
        "\n",
        "# Save the model\n",
        "torch.save(model_ft.state_dict(), '/content/drive/My Drive/Capstone/model2_resnet50.pth')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "eioNaPjLllMB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59ef561e-55c2-4951-c1fd-d90130426df5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test set: Average loss: 1.9508, Accuracy: 0.3372\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import models\n",
        "import copy\n",
        "\n",
        "# Function to evaluate the model on the test set\n",
        "def evaluate_model(model, dataloader, criterion):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            running_corrects += torch.sum(preds == labels.data)\n",
        "            total += labels.size(0)\n",
        "\n",
        "    final_loss = running_loss / total\n",
        "    final_acc = running_corrects.double() / total\n",
        "    print(f'Test set: Average loss: {final_loss:.4f}, Accuracy: {final_acc:.4f}')\n",
        "\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Check if GPU is available and set the device accordingly\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Initialize the model architecture\n",
        "model_ft = models.resnet50(pretrained=True)\n",
        "num_ftrs = model_ft.fc.in_features\n",
        "# Update the next line with the correct number of output classes\n",
        "model_ft.fc = nn.Linear(num_ftrs, 15)\n",
        "model_ft = model_ft.to(device)\n",
        "\n",
        "# Load the trained model state\n",
        "model_ft.load_state_dict(torch.load('/content/drive/My Drive/Capstone/model2_resnet50.pth'))\n",
        "\n",
        "# Now you can evaluate your model\n",
        "evaluate_model(model_ft, test_loader, criterion)\n",
        "\n",
        "# # Load the trained model (if not already loaded)\n",
        "# model_ft.load_state_dict(torch.load('/content/drive/My Drive/Capstone/model2_resnet50.pth'))\n",
        "# #model.load_state_dict(torch.load('/content/drive/My Drive/Capstone/model_resnet50.pth', map_location=device))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## THIRD MODEL"
      ],
      "metadata": {
        "id": "k2otHMsKBZB8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S_Sj2w8ullJ3"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import models\n",
        "import copy\n",
        "\n",
        "# Assume the rest of the code above this is the same\n",
        "\n",
        "# Initialize the model for fine-tuning\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = models.resnet50(pretrained=True)\n",
        "\n",
        "# Unfreeze all layers\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Linear(num_ftrs, len(class_to_idx))  # Adjust to your number of classes\n",
        "model = model.to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# AdamW optimizer with weight decay\n",
        "optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
        "\n",
        "# Learning rate scheduler\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.3)\n",
        "\n",
        "# Training function - same as before, with modification to scheduler step\n",
        "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()\n",
        "            else:\n",
        "                model.eval()\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data\n",
        "            for inputs, labels in (train_loader if phase == 'train' else val_loader):\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # Zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # Forward\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    # Backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # Statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            epoch_loss = running_loss / len(train_loader.dataset if phase == 'train' else val_loader.dataset)\n",
        "            epoch_acc = running_corrects.double() / len(train_loader.dataset if phase == 'train' else val_loader.dataset)\n",
        "\n",
        "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
        "\n",
        "            # Step the scheduler if in validation phase\n",
        "            if phase == 'val':\n",
        "                scheduler.step(epoch_loss)\n",
        "\n",
        "            # Deep copy the model\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "    print(f'Best val Acc: {best_acc:.4f}')\n",
        "    # Load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model\n",
        "\n",
        "# Train the model\n",
        "model_ft = train_model(model, criterion, optimizer, scheduler, num_epochs=25)\n",
        "\n",
        "# Save the model\n",
        "torch.save(model_ft.state_dict(), '/content/drive/My Drive/Capstone/model3_resnet50.pth')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vVqxBIRyllHo"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}